/home/brett/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO:root:Epoch 1
Process Process-1:
Traceback (most recent call last):
  File "/home/brett/anaconda3/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/brett/anaconda3/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/brett/Desktop/tutorials/ijepa/ijepa/main.py", line 54, in process_main
    app_main(args=params)
  File "/home/brett/Desktop/tutorials/ijepa/ijepa/src/supervised_train.py", line 376, in main
    train_loss, _new_lr = train_step()
  File "/home/brett/Desktop/tutorials/ijepa/ijepa/src/supervised_train.py", line 362, in train_step
    with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):
  File "/home/brett/anaconda3/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py", line 34, in __init__
    super().__init__(
  File "/home/brett/anaconda3/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 306, in __init__
    raise RuntimeError(
RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.